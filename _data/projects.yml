- id: gss
  title: "Named Entity Recognition as Structured Span Prediction"
  authors:
    - name: Urchade Zaratiana
      website: urchade.github.io
    - name: Nadi Tomeh
      website: https://lipn.univ-paris13.fr/~tomeh/
    - name: Pierre Holat
      website: https://lipn.univ-paris13.fr/~holat/research.html
    - name: Thierry Charnois
      website: https://lipn.univ-paris13.fr/~charnois/
  type: workshop
  venue: "Workshop on Unimodal and Multimodal Induction of Linguistic Structures @ EMNLP 2022"
  abstract: >
    Named Entity Recognition (NER) is an important task in Natural Language Processing with applications in many domains. While the dominant paradigm of NER is sequence labelling, span-based approaches have become very popular in recent times but are less well understood. In this work, we study different aspects of span-based NER, namely the span representation, learning strategy, and decoding algorithms to avoid span overlap. We also propose an exact algorithm that efficiently finds the set of non-overlapping spans that maximizes a global score, given a list of candidate spans. We performed our study on three benchmark NER datasets from different domains. We make our code publicly available at https://github.com/urchade/span-structured-prediction.
  paper: https://aclanthology.org/2022.umios-1.2.pdf
  
- id: gss
  title: "Global Span Selection for Named Entity Recognition"
  authors:
    - name: Urchade Zaratiana
      website: urchade.github.io
    - name: Niama El Khbir
    - name: Pierre Holat
      website: https://lipn.univ-paris13.fr/~holat/research.html
    - name: Nadi Tomeh
      website: https://lipn.univ-paris13.fr/~tomeh/
    - name: Thierry Charnois
      website: https://lipn.univ-paris13.fr/~charnois/
  type: workshop
  venue: "Workshop on Unimodal and Multimodal Induction of Linguistic Structures @ EMNLP 2022"
  abstract: >
    Named Entity Recognition (NER) is an important task in Natural Language Processing with applications in many domains. In this paper, we describe a novel approach to named entity recognition, in which we output a set of spans (i.e., segmentations) by maximizing a global score. During training, we optimize our model by maximizing the probability of the gold segmentation. During inference, we use dynamic programming to select the best segmentation under a linear time complexity. We prove that our approach outperforms CRF and semi-CRF models for Named Entity Recognition. We will make our code publicly available.
  paper: https://aclanthology.org/2022.umios-1.2.pdf
  
- id: dyref
  title: "DyReF: Extractive Question Answering with Dynamic Query Representation for Free"
  authors:
    - name: Urchade Zaratiana
      website: urchade.github.io
    - name: Niama El Khbir
    - name: Pierre Holat
      website: https://lipn.univ-paris13.fr/~holat/research.html
    - name: Nadi Tomeh
      website: https://lipn.univ-paris13.fr/~tomeh/
    - name: Thierry Charnois
      website: https://lipn.univ-paris13.fr/~charnois/
  type: workshop
  venue: "Workshop on Transfer Learning for Natural Language Processing"
  abstract: >
    Extractive QA is an important NLP task with numerous real-world applications. The most common method
    for extractive QA is to encode the input sequence with a pretrained Transformer such as BERT, and
    then compute the probability of the start and end positions of span answers using two leaned query
    vectors. This method has been shown to be effective and hard to outperform. However, the query
    vectors are static, meaning they are the same regardless of the input, which can be a challenging
    issue in improving the model's performance. To address this problem, we propose 
    \texttt{DyReF} (\texttt{Dy}namic \texttt{Re}presentation for \texttt{F}ree), a model that dynamically
    learns query vectors for free, i.e. without adding any parameters, by concatenating the query vectors
    with the embeddings of the input tokens of the Transformer layers. In this way, the query vectors can
    aggregate information from the source sentence and adapt to the question, while the representations of
    the input tokens are also dependent on the queries, allowing for better task specialization. 
    We demonstrate empirically that our simple approach outperforms strong baseline in a variety
    of extractive question answering benchmark datasets. The code is publicly 
    available at \url{https://github.com/urchade/DyReF}.
  paper: assets/dyreff.pdf

- id: dyrex
  title: "DyReX: Dynamic Query Representation for Extractive Question Answering"
  authors:
    - name: Urchade Zaratiana
      website: urchade.github.io
    - name: Niama El Khbir
    - name: Dennis Núñez
      website: https://dennishnf.com
    - name: Pierre Holat
      website: https://lipn.univ-paris13.fr/~holat/research.html
    - name: Nadi Tomeh
      website: https://lipn.univ-paris13.fr/~tomeh/
    - name: Thierry Charnois
      website: https://lipn.univ-paris13.fr/~charnois/
  type: workshop
  venue: "2nd Workshop on Efficient Natural Language and Speech Processing @ NeurIPS 2022"
  abstract: >
    Extractive question answering (ExQA) is an essential task for Natural Language Processing. 
    The dominant approach to ExQA is one that represents the input sequence tokens (question and passage)
    with a pre-trained transformer, then uses two learned query vectors to compute distributions over
    the start and end answer span positions. These query vectors lack the context of the inputs,
    which can be a bottleneck for the model performance. To address this problem, we propose \textit{DyREx},
    a generalization of the \textit{vanilla} approach where we dynamically compute query vectors given
    the input, using an attention mechanism through transformer layers. Empirical observations
    demonstrate that our approach consistently improves the performance over the standard one.
    The code and accompanying files for running the experiments are available at \url{https://github.com/urchade/DyReX}.
  paper: https://arxiv.org/abs/2210.15048

- id: impact
  title: "GNNer: Reducing Overlapping in Span-based NER Using Graph Neural Networks"
  authors:
    - name: Urchade Zaratiana
      website: urchade.github.io
    - name: Nadi Tomeh
      website: https://lipn.univ-paris13.fr/~tomeh/
    - name: Pierre Holat
      website: https://lipn.univ-paris13.fr/~holat/research.html
    - name: Thierry Charnois
      website: https://lipn.univ-paris13.fr/~charnois/
  type: workshop
  venue: "ACL SRW 2022"
  abstract: >
    There are two main paradigms for Named Entity Recognition (NER): sequence labelling
    and span classification. Sequence labelling aims to assign a label to each word in
    an input text using, for example, BIO (Begin, Inside and Outside) tagging, while span
    classification involves enumerating all possible spans in a text and classifying them 
    into their labels. In contrast to sequence labelling, unconstrained span-based methods
    tend to assign entity labels to overlapping spans, which is generally undesirable,
    especially for NER tasks without nested entities. Accordingly, we propose GNNer, a
    framework that uses Graph Neural Networks to enrich the span representation to reduce
    the number of overlapping spans during prediction. Our approach reduces the number of
    overlapping spans compared to strong baseline while maintaining competitive metric
    performance. Code is available at https://github.com/urchade/GNNer.
  paper: https://aclanthology.org/2022.acl-srw.9
  poster: assets/gnner-poster.pdf

- id: ida
  title: "Towards Automation of Topic Taxonomy Construction"
  authors:
    - name: Yann Dauxais
      website: http://dauxais.fr
    - name: Urchade Zaratiana
      website: urchade.github.io
    - name: Matthieu Laneuville
    - name: Simon David Hernandez
    - name: Pierre Holat
      website: https://lipn.univ-paris13.fr/~holat/research.html
    - name: Charlie Grosman
  type: conference
  venue: "IDA 2022: Advances in Intelligent Data Analysis"
  abstract: >
    The automation of taxonomy construction has increased in popularity recently. Such an interest for the domain
    has been motivated by the large number of new scientific papers published each year that implies a growing
    difficulty in following the new topics of the different scientific domains and their importance in the topic
    hierarchy. In this paper, we propose a way to automatically construct topic taxonomies from millions of scientific
    article abstracts and ways to automatically evaluate this construction. While, to our knowledge, other approaches
    rely on pipelines of models and human evaluation to validate them, we chose to rely on simple models that are
    easier to evaluate automatically and, thus, promote the improvement of our models thanks to a large number of 
    iterations. The contribution of this paper is threefold: 1) the proposition of a new method to construct taxonomies
    from a large set of scientific papers, 2) a method to precompile taxonomy information into matrices that will be
    quickly queried, and 3) an objective method to automatically evaluate the constructed taxonomies without requiring
    human evaluation.
  paper: https://link.springer.com/chapter/10.1007/978-3-031-01333-1_3
